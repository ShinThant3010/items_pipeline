import json
from io import BytesIO
from datetime import date, datetime, time
from typing import Any

import numpy as np
from google.cloud import storage


def parse_gcs_prefix(prefix: str, *, field_name: str = "gcs_prefix") -> tuple[str, str]:
    """
    Parse a GCS prefix into bucket and path.
    """
    if not prefix.startswith("gs://"):
        raise ValueError(f"{field_name} must start with gs://")
    remainder = prefix[5:]
    bucket, _, path = remainder.partition("/")
    if not bucket or not path:
        raise ValueError(f"{field_name} must include bucket and folder path")
    return bucket, path


def write_to_gcs(
    gcs_prefix: str,
    items: list[dict[str, Any]],
    *,
    filename: str = "part-00000",
    file_type: str = "json",
) -> str:
    """
    Write items to GCS as a single file.
    """
    bucket_name, path = parse_gcs_prefix(gcs_prefix, field_name="gcs_output_prefix")

    clean_filename = filename.strip() or "part-00000"
    clean_file_type = file_type.strip().lstrip(".") or "json"
    blob_name = f"{path.rstrip('/')}/{clean_filename}.{clean_file_type}"
    payload = (
        "\n".join(json.dumps(item, ensure_ascii=True, default=_json_default) for item in items)
        + "\n"
    )

    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    bucket.blob(blob_name).upload_from_string(payload, content_type="application/json")

    return f"gs://{bucket_name}/{blob_name}"


def _json_default(value: Any) -> Any:
    if isinstance(value, (datetime, date, time)):
        return value.isoformat()
    return str(value)


def load_data_from_gcs_prefix(
    gcs_prefix: str,
    *,
    field_name: str = "gcs_prefix",
    file_type: str = "json",
) -> list[Any]:
    """
    Load data items from GCS prefix. 
    Supports json, txt, and npy file types. 
    """
    supported = {"json", "txt", "npy"}
    target_type = file_type.strip().lstrip(".").lower() or "json"

    # Validate file type
    if target_type not in supported:
        raise ValueError(f"Unsupported file_type `{file_type}`. Supported: json, txt, npy")

    # Parse GCS prefix
    bucket_name, prefix = parse_gcs_prefix(gcs_prefix, field_name=field_name)
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)

    # Load items
    items: list[Any] = []
    for blob in bucket.list_blobs(prefix=prefix.rstrip("/") + "/"):
        if blob.name.endswith("/"):
            continue
        ext = blob.name.rsplit(".", 1)[-1].lower() if "." in blob.name else ""
        if ext != target_type:
            continue

        # Load based on file type
        if target_type == "json":
            content = blob.download_as_text()
            for line in content.splitlines():
                line = line.strip()
                if not line:
                    continue
                try:
                    items.append(json.loads(line))
                except json.JSONDecodeError:
                    # Fall back to whole-file JSON parse if not jsonl.
                    parsed = json.loads(content)
                    if isinstance(parsed, list):
                        items.extend(parsed)
                    else:
                        items.append(parsed)
                    break
            continue

        if target_type == "txt":
            content = blob.download_as_text()
            for line in content.splitlines():
                line = line.strip()
                if line:
                    items.append(line)
            continue

        if target_type == "npy":
            raw = blob.download_as_bytes()
            array = np.load(BytesIO(raw), allow_pickle=True)
            items.append(array.tolist())
            continue

    return items
